BERT文本编码与相似度计算技术方案
一、整体流程
1.输入文本预处理：对原始文本进行分词、添加特殊标记（如[CLS]、[SEP]）。
2.BERT编码：将文本输入BERT模型，获取句向量表示。
3.相似度计算：通过向量相似度算法（如余弦相似度）计算文本间的相似度。
4.整体框架：文本输入→预处理→BERT编码→向量提取→相似度计算→结果输出。
二、详细步骤
2.1文本预处理
1.使用BERT的分词器（Tokenizer）将文本转换为词符（Token）序列。
2.添加[CLS]（分类标记）和[SEP]（分隔标记），并进行填充（Padding）或截断。
3.生成注意力掩码（Attention Mask）以区分实际词符与填充符。
2.2BERT编码
1.将词符序列、注意力掩码输入预训练的BERT模型（如bert-base-uncased）。
2.提取[CLS]标记的输出向量作为整个句子的语义表示（句向量），或对最后一层所有词符向量进行均值池化。
2.3相似度计算
1.将两个文本的句向量归一化（L2归一化），消除长度影响、标准化向量空间。
2.通过计算两个向量夹角的余弦值来衡量方向相似性计算余弦相似度。
3.相似度值范围[-1,1]，越接近1表示语义越相似。
三、流程图
